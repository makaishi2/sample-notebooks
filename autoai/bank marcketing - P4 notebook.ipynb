{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# #Licensed Materials - Property of IBM\n",
    "#(C) Copyright IBM Corp. 2019\n",
    "#US Government Users Restricted Rights - Use, duplication disclosure restricted\n",
    "#by GSA ADP Schedule Contract with IBM Corp.\n",
    "################################################################################\n",
    "\n",
    "The auto-generated notebooks are subject to the International License Agreement for Non-Warranted Programs (or equivalent) and License Information document for Watson Studio Auto-generated Notebook (\"License Terms\"), such agreements located in the link below.\n",
    "Specifically, the Source Components and Sample Materials clause included in the License Information document for\n",
    "Watson Studio Auto-generated Notebook applies to the auto-generated notebooks. \n",
    "By downloading, copying, accessing, or otherwise using the materials, you agree to the License Terms.\n",
    "http://www14.software.ibm.com/cgi-bin/weblap/lap.pl?li_formnum=L-AMCU-BHU2B7&title=IBM%20Watson%20Studio%20Auto-generated%20Notebook%20V2.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IBM AutoAI Auto-Generated Notebook v1.11.7\n",
    "### Representing Pipeline: P4 from run c587fdb8-5883-4f67-ab4c-fdc2104055cf\n",
    "\n",
    "**Note**: Notebook code generated using AutoAI will execute successfully.\n",
    "If code is modified or reordered, there is no guarantee it will successfully execute.\n",
    "This pipeline is optimized for the original dataset.  The pipeline may fail or produce sub-optimium results if used with different data.\n",
    "For different data, please consider returning to AutoAI Experiements to generate a new pipeline.\n",
    "Please read our documentation for more information:  \n",
    "(IBM Cloud Platform) https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/autoai-notebook.html .  \n",
    "(IBM Cloud Pak For Data) https://www.ibm.com/support/knowledgecenter/SSQNUZ_3.0.0/wsj/analyze-data/autoai-notebook.html .  \n",
    "\n",
    "Before modifying the pipeline or trying to re-fit the pipeline, consider:  \n",
    "The notebook converts dataframes to numpy arrays before fitting the pipeline (a current restriction of the preprocessor pipeline).\n",
    "The known_values_list is passed by reference and populated with categorical values during fit of the preprocessing pipeline.  Delete its members before re-fitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda3/lib/python3.7/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "#attempt import of autoai_libs and install if missing\n",
    "try:\n",
    "    import autoai_libs\n",
    "except Exception as e:\n",
    "    print('attempting to install missing autoai_libs from pypi, this may take tens of seconds to complete.')\n",
    "    import subprocess\n",
    "    try:\n",
    "        # attempt to install missing autoai-libs from pypi\n",
    "        out = subprocess.check_output('pip install autoai-libs', shell=True)\n",
    "        for line in out.splitlines():\n",
    "            print(line)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "try:\n",
    "    import autoai_libs\n",
    "except Exception as e:\n",
    "    print('attempting to install missing autoai_libs from local filesystem, this may take tens of seconds to complete.')\n",
    "    import subprocess\n",
    "    # attempt to install missing autoai-libs from local filesystem\n",
    "    try:\n",
    "        out = subprocess.check_output('pip install .', shell=True, cwd='software/autoai_libs')\n",
    "        for line in out.splitlines():\n",
    "            print(line)\n",
    "        import autoai_libs\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "import sklearn\n",
    "try:\n",
    "    import xgboost\n",
    "except:\n",
    "    print('xgboost, if needed, will be installed and imported later')\n",
    "try:\n",
    "    import lightgbm\n",
    "except:\n",
    "    print('lightgbm, if needed, will be installed and imported later')\n",
    "from sklearn.cluster import FeatureAgglomeration\n",
    "import numpy\n",
    "from numpy import inf, nan, dtype, mean\n",
    "from autoai_libs.sklearn.custom_scorers import CustomScorers\n",
    "from autoai_libs.cognito.transforms.transform_utils import TExtras, FC\n",
    "from autoai_libs.transformers.exportable import *\n",
    "from autoai_libs.utils.exportable_utils import *\n",
    "from sklearn.pipeline import Pipeline\n",
    "known_values_list=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compose a decorator to assist pipeline instantiation via import of modules and installation of packages\n",
    "def decorator_retries(func):\n",
    "    def install_import_retry(*args, **kwargs):\n",
    "        retries = 0\n",
    "        successful = False\n",
    "        failed_retries = 0\n",
    "        while retries < 100 and failed_retries < 10 and not successful:\n",
    "            retries += 1\n",
    "            failed_retries += 1\n",
    "            try:\n",
    "                result = func(*args, **kwargs)\n",
    "                successful = True\n",
    "            except Exception as e:\n",
    "                estr = str(e)\n",
    "                if estr.startswith('name ') and estr.endswith(' is not defined'):\n",
    "                    try:\n",
    "                        import importlib\n",
    "                        module_name = estr.split(\"'\")[1]\n",
    "                        module = importlib.import_module(module_name)\n",
    "                        globals().update({module_name: module})\n",
    "                        print('import successful for ' + module_name)\n",
    "                        failed_retries -= 1\n",
    "                    except Exception as import_failure:\n",
    "                        print('import of ' + module_name + ' failed with: ' + str(import_failure))\n",
    "                        import subprocess\n",
    "                        print('attempting pip install of ' + module_name)\n",
    "                        process = subprocess.Popen('pip install ' + module_name, shell=True)\n",
    "                        process.wait()\n",
    "                        try:\n",
    "                            print('re-attempting import of ' + module_name)\n",
    "                            module = importlib.import_module(module_name)\n",
    "                            globals().update({module_name: module})\n",
    "                            print('import successful for ' + module_name)\n",
    "                            failed_retries -= 1\n",
    "                        except Exception as import_or_installation_failure:\n",
    "                            print('failure installing and/or importing ' + module_name + ' error was: ' + str(\n",
    "                                import_or_installation_failure))\n",
    "                            raise (ModuleNotFoundError('Missing package in environment for ' + module_name +\n",
    "                                                       '? Try import and/or pip install manually?'))\n",
    "                elif type(e) is AttributeError:\n",
    "                    if 'module ' in estr and ' has no attribute ' in estr:\n",
    "                        pieces = estr.split(\"'\")\n",
    "                        if len(pieces) == 5:\n",
    "                            try:\n",
    "                                import importlib\n",
    "                                print('re-attempting import of ' + pieces[3] + ' from ' + pieces[1])\n",
    "                                module = importlib.import_module('.' + pieces[3], pieces[1])\n",
    "                                failed_retries -= 1\n",
    "                            except:\n",
    "                                print('failed attempt to import ' + pieces[3])\n",
    "                                raise (e)\n",
    "                        else:\n",
    "                            raise (e)\n",
    "                else:\n",
    "                    raise (e)\n",
    "        if successful:\n",
    "            print('Pipeline successfully instantiated')\n",
    "        else:\n",
    "            raise (ModuleNotFoundError(\n",
    "                'Remaining missing imports/packages in environment? Retry cell and/or try pip install manually?'))\n",
    "        return result\n",
    "    return install_import_retry\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Compose Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re-attempting import of ensemble from sklearn\n",
      "Pipeline successfully instantiated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# metadata necessary to replicate AutoAI scores with the pipeline\n",
    "_input_metadata = {'run_uid': 'c587fdb8-5883-4f67-ab4c-fdc2104055cf', 'pn': 'P4', 'data_source': '', 'target_label_name': 'y', 'learning_type': 'classification', 'optimization_metric': 'accuracy', 'random_state': 33, 'cv_num_folds': 3, 'holdout_fraction': 0.1, 'pos_label': 'yes'}\n",
    "\n",
    "# define a function to compose the pipeline, and invoke it\n",
    "@decorator_retries\n",
    "def compose_pipeline():\n",
    "    import numpy\n",
    "    from numpy import nan, dtype, mean\n",
    "    #\n",
    "    # composing steps for toplevel Pipeline\n",
    "    #\n",
    "    _input_metadata = {'run_uid': 'c587fdb8-5883-4f67-ab4c-fdc2104055cf', 'pn': 'P4', 'data_source': '', 'target_label_name': 'y', 'learning_type': 'classification', 'optimization_metric': 'accuracy', 'random_state': 33, 'cv_num_folds': 3, 'holdout_fraction': 0.1, 'pos_label': 'yes'}\n",
    "    steps = []\n",
    "    #\n",
    "    # composing steps for preprocessor Pipeline\n",
    "    #\n",
    "    preprocessor__input_metadata = None\n",
    "    preprocessor_steps = []\n",
    "    #\n",
    "    # composing steps for preprocessor_features FeatureUnion\n",
    "    #\n",
    "    preprocessor_features_transformer_list = []\n",
    "    #\n",
    "    # composing steps for preprocessor_features_categorical Pipeline\n",
    "    #\n",
    "    preprocessor_features_categorical__input_metadata = None\n",
    "    preprocessor_features_categorical_steps = []\n",
    "    preprocessor_features_categorical_steps.append(('cat_column_selector', autoai_libs.transformers.exportable.NumpyColumnSelector(columns=[0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 12, 14, 15])))\n",
    "    preprocessor_features_categorical_steps.append(('cat_compress_strings', autoai_libs.transformers.exportable.CompressStrings(activate_flag=True, compress_type='hash', dtypes_list=['int_num', 'char_str', 'char_str', 'char_str', 'char_str', 'char_str', 'char_str', 'char_str', 'int_num', 'char_str', 'int_num', 'int_num', 'char_str'], missing_values_reference_list=['', '-', '?', nan], misslist_list=[[], [], [], [], [], [], [], [], [], [], [], [], []])))\n",
    "    preprocessor_features_categorical_steps.append(('cat_missing_replacer', autoai_libs.transformers.exportable.NumpyReplaceMissingValues(filling_values=nan, missing_values=[])))\n",
    "    preprocessor_features_categorical_steps.append(('cat_unknown_replacer', autoai_libs.transformers.exportable.NumpyReplaceUnknownValues(filling_values=nan, filling_values_list=[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], known_values_list=known_values_list, missing_values_reference_list=['', '-', '?', nan])))\n",
    "    preprocessor_features_categorical_steps.append(('boolean2float_transformer', autoai_libs.transformers.exportable.boolean2float(activate_flag=True)))\n",
    "    preprocessor_features_categorical_steps.append(('cat_imputer', autoai_libs.transformers.exportable.CatImputer(activate_flag=True, missing_values=nan, sklearn_version_family='20', strategy='most_frequent')))\n",
    "    preprocessor_features_categorical_steps.append(('cat_encoder', autoai_libs.transformers.exportable.CatEncoder(activate_flag=True, categories='auto', dtype=numpy.float64, encoding='ordinal', handle_unknown='error', sklearn_version_family='20')))\n",
    "    preprocessor_features_categorical_steps.append(('float32_transformer', autoai_libs.transformers.exportable.float32_transform(activate_flag=True)))\n",
    "    # assembling preprocessor_features_categorical_ Pipeline\n",
    "    preprocessor_features_categorical_pipeline = sklearn.pipeline.Pipeline(steps=preprocessor_features_categorical_steps)\n",
    "    preprocessor_features_transformer_list.append(('categorical', preprocessor_features_categorical_pipeline))\n",
    "    #\n",
    "    # composing steps for preprocessor_features_numeric Pipeline\n",
    "    #\n",
    "    preprocessor_features_numeric__input_metadata = None\n",
    "    preprocessor_features_numeric_steps = []\n",
    "    preprocessor_features_numeric_steps.append(('num_column_selector', autoai_libs.transformers.exportable.NumpyColumnSelector(columns=[5, 11, 13])))\n",
    "    preprocessor_features_numeric_steps.append(('num_floatstr2float_transformer', autoai_libs.transformers.exportable.FloatStr2Float(activate_flag=True, dtypes_list=['int_num', 'int_num', 'int_num'], missing_values_reference_list=[])))\n",
    "    preprocessor_features_numeric_steps.append(('num_missing_replacer', autoai_libs.transformers.exportable.NumpyReplaceMissingValues(filling_values=nan, missing_values=[])))\n",
    "    preprocessor_features_numeric_steps.append(('num_imputer', autoai_libs.transformers.exportable.NumImputer(activate_flag=True, missing_values=nan, strategy='median')))\n",
    "    preprocessor_features_numeric_steps.append(('num_scaler', autoai_libs.transformers.exportable.OptStandardScaler(num_scaler_copy=None, num_scaler_with_mean=None, num_scaler_with_std=None, use_scaler_flag=False)))\n",
    "    preprocessor_features_numeric_steps.append(('float32_transformer', autoai_libs.transformers.exportable.float32_transform(activate_flag=True)))\n",
    "    # assembling preprocessor_features_numeric_ Pipeline\n",
    "    preprocessor_features_numeric_pipeline = sklearn.pipeline.Pipeline(steps=preprocessor_features_numeric_steps)\n",
    "    preprocessor_features_transformer_list.append(('numeric', preprocessor_features_numeric_pipeline))\n",
    "    # assembling preprocessor_features_ FeatureUnion\n",
    "    preprocessor_features_pipeline = sklearn.pipeline.FeatureUnion(transformer_list=preprocessor_features_transformer_list)\n",
    "    preprocessor_steps.append(('features', preprocessor_features_pipeline))\n",
    "    preprocessor_steps.append(('permuter', autoai_libs.transformers.exportable.NumpyPermuteArray(axis=0, permutation_indices=[0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 12, 14, 15, 5, 11, 13])))\n",
    "    # assembling preprocessor_ Pipeline\n",
    "    preprocessor_pipeline = sklearn.pipeline.Pipeline(steps=preprocessor_steps)\n",
    "    steps.append(('preprocessor', preprocessor_pipeline))\n",
    "    #\n",
    "    # composing steps for cognito Pipeline\n",
    "    #\n",
    "    cognito__input_metadata = None\n",
    "    cognito_steps = []\n",
    "    cognito_steps.append(('0', autoai_libs.cognito.transforms.transform_utils.TA1(fun=numpy.tan, name='tan', datatypes=['float'], feat_constraints=[autoai_libs.utils.fc_methods.is_not_categorical], tgraph=None, apply_all=True, col_names=['age', 'job', 'marital', 'education', 'default', 'balance', 'housing', 'loan', 'contact', 'day', 'month', 'duration', 'campaign', 'pdays', 'previous', 'poutcome'], col_dtypes=[dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32')], col_as_json_objects=None)))\n",
    "    cognito_steps.append(('1', autoai_libs.cognito.transforms.transform_utils.FS1(cols_ids_must_keep=range(0, 16), additional_col_count_to_keep=15, ptype='classification')))\n",
    "    cognito_steps.append(('2', autoai_libs.cognito.transforms.transform_utils.TA1(fun=numpy.sqrt, name='sqrt', datatypes=['numeric'], feat_constraints=[autoai_libs.utils.fc_methods.is_non_negative, autoai_libs.utils.fc_methods.is_not_categorical], tgraph=None, apply_all=True, col_names=['age', 'job', 'marital', 'education', 'default', 'balance', 'housing', 'loan', 'contact', 'day', 'month', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'tan(age)', 'tan(balance)', 'tan(day)', 'tan(duration)', 'tan(campaign)', 'tan(pdays)', 'tan(previous)'], col_dtypes=[dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32')], col_as_json_objects=None)))\n",
    "    cognito_steps.append(('3', autoai_libs.cognito.transforms.transform_utils.FS1(cols_ids_must_keep=range(0, 16), additional_col_count_to_keep=15, ptype='classification')))\n",
    "    cognito_steps.append(('4', autoai_libs.cognito.transforms.transform_utils.TAM(tans_class=sklearn.decomposition.pca.PCA(copy=True, iterated_power='auto', n_components=None, random_state=None, svd_solver='auto', tol=0.0, whiten=False), name='pca', tgraph=None, apply_all=True, col_names=['age', 'job', 'marital', 'education', 'default', 'balance', 'housing', 'loan', 'contact', 'day', 'month', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'tan(day)', 'tan(campaign)', 'tan(previous)', 'sqrt(age)', 'sqrt(balance)', 'sqrt(day)', 'sqrt(duration)', 'sqrt(campaign)', 'sqrt(pdays)', 'sqrt(previous)', 'sqrt(tan(age))', 'sqrt(tan(duration))', 'sqrt(tan(campaign))', 'sqrt(tan(pdays))', 'sqrt(tan(previous))'], col_dtypes=[dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32')], col_as_json_objects=None)))\n",
    "    cognito_steps.append(('5', autoai_libs.cognito.transforms.transform_utils.FS1(cols_ids_must_keep=range(0, 16), additional_col_count_to_keep=15, ptype='classification')))\n",
    "    # assembling cognito_ Pipeline\n",
    "    cognito_pipeline = sklearn.pipeline.Pipeline(steps=cognito_steps)\n",
    "    steps.append(('cognito', cognito_pipeline))\n",
    "    steps.append(('estimator', sklearn.ensemble.gradient_boosting.GradientBoostingClassifier(criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance', max_depth=3, max_features=0.9950479310055053, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=0.01, min_samples_split=0.44841798790040854, min_weight_fraction_leaf=0.0, n_estimators=73, n_iter_no_change=None, presort='auto', random_state=33, subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=False)))\n",
    "    # assembling  Pipeline\n",
    "    pipeline = sklearn.pipeline.Pipeline(steps=steps)\n",
    "    return pipeline\n",
    "pipeline = compose_pipeline()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Extract needed parameter values from AutoAI run metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Metadata used in retrieving data and computing metrics.  Customize as necessary for your environment.\n",
    "#data_source='replace_with_path_and_csv_filename'\n",
    "target_label_name = _input_metadata['target_label_name']\n",
    "learning_type = _input_metadata['learning_type']\n",
    "optimization_metric = _input_metadata['optimization_metric']\n",
    "random_state = _input_metadata['random_state']\n",
    "cv_num_folds = _input_metadata['cv_num_folds']\n",
    "holdout_fraction = _input_metadata['holdout_fraction']\n",
    "if 'data_provenance' in _input_metadata:\n",
    "    data_provenance = _input_metadata['data_provenance']\n",
    "else:\n",
    "    data_provenance = None\n",
    "if 'pos_label' in _input_metadata and learning_type == 'classification':\n",
    "    pos_label = _input_metadata['pos_label']\n",
    "else:\n",
    "    pos_label = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create dataframe from dataset in IBM Cloud Object Storage or IBM Cloud Pak For Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# @hidden_cell\n",
    "# The following code contains the credentials for a file in your IBM Cloud Object Storage.\n",
    "# You might want to remove those credentials before you share your notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully loaded dataframe using encoding = UTF-8\n"
     ]
    }
   ],
   "source": [
    "#  Read the data as a dataframe\n",
    "import pandas as pd\n",
    "\n",
    "csv_encodings=['UTF-8','Latin-1'] # supplement list of encodings as necessary for your data\n",
    "df = None\n",
    "readable = 'bank-train.csv'  # if automatic detection fails, you can supply a filename here\n",
    "\n",
    "# First, obtain a readable object\n",
    "# IBM Cloud Object Storage data access\n",
    "# Assumes COS credentials are in a dictionary named 'credentials_0'\n",
    "cos_credentials = df = globals().get('credentials_0')       \n",
    "if readable is None and cos_credentials is not None:\n",
    "    print('accessing data via IBM Cloud Object Storage')\n",
    "    try:\n",
    "        import types\n",
    "        from botocore.client import Config\n",
    "        import ibm_boto3\n",
    "\n",
    "        def __iter__(self): return 0\n",
    "\n",
    "        if 'SERVICE_NAME' not in cos_credentials:  # in case of Studio-supplied credentials for a different dataset\n",
    "            cos_credentials['SERVICE_NAME'] = 's3'\n",
    "        client = ibm_boto3.client(service_name=cos_credentials['SERVICE_NAME'],\n",
    "            ibm_api_key_id=cos_credentials['IBM_API_KEY_ID'],\n",
    "            ibm_auth_endpoint=cos_credentials['IBM_AUTH_ENDPOINT'],\n",
    "            config=Config(signature_version='oauth'),\n",
    "            endpoint_url=cos_credentials['ENDPOINT'])\n",
    "\n",
    "        try:\n",
    "            readable = client.get_object(Bucket=cos_credentials['BUCKET'],Key=cos_credentials['FILE'])['Body']\n",
    "            # add missing __iter__ method, so pandas accepts readable as file-like object\n",
    "            if not hasattr(readable, \"__iter__\"): readable.__iter__ = types.MethodType( __iter__, readable )\n",
    "        except Exception as cos_access_exception:\n",
    "            print('unable to access data object in cloud object storage with credentials supplied')\n",
    "    except Exception as cos_exception:\n",
    "        print('unable to create client for cloud object storage')\n",
    "\n",
    "# IBM Cloud Pak for Data data access\n",
    "project_filename = globals().get('project_filename')       \n",
    "if readable is None and 'credentials_0' in globals() and 'ASSET_ID' in credentials_0:\n",
    "    project_filename = credentials_0['ASSET_ID']\n",
    "if project_filename is not None:\n",
    "    print('attempting project_lib access to ' + str(project_filename))\n",
    "    try:\n",
    "        from project_lib import Project\n",
    "        project = Project.access()\n",
    "        storage_credentials = project.get_storage_metadata()\n",
    "        readable = project.get_file(project_filename)\n",
    "    except Exception as project_exception:\n",
    "        print('unable to access data using the project_lib interface and filename supplied')\n",
    "\n",
    "# Use data_provenance as filename if other access mechanisms are unsuccessful\n",
    "if readable is None and type(data_provenance) is str:\n",
    "    print('attempting to access local file using path and name ' + data_provenance)\n",
    "    readable = data_provenance\n",
    "\n",
    "# Second, use pd.read_csv to read object, iterating over list of csv_encodings until successful\n",
    "if readable is not None:\n",
    "    for encoding in csv_encodings:\n",
    "        try:\n",
    "            df = pd.read_csv(readable, encoding=encoding)\n",
    "            print('successfully loaded dataframe using encoding = ' + str(encoding))\n",
    "            break\n",
    "        except Exception as exception_csv:\n",
    "            print('unable to read csv using encoding ' + str(encoding))\n",
    "            print('handled error was ' + str(exception_csv))\n",
    "    if df is None:\n",
    "        print('unable to read file/object as a dataframe using supplied csv_encodings ' + str(csv_encodings))\n",
    "        print(\"Please use 'insert to code' on data panel to load dataframe.\")\n",
    "        raise(ValueError('unable to read file/object as a dataframe using supplied csv_encodings ' + str(csv_encodings)))\n",
    "\n",
    "if df is None:\n",
    "    print('Unable to access bucket/file in IBM Cloud Object Storage or asset in IBM Cloud Pak for Data with the parameters supplied.')\n",
    "    print('This is abnormal, but proceeding assuming the notebook user will supply a dataframe by other means.')\n",
    "    print(\"Please use 'insert to code' on data panel to load dataframe.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows whose target is not defined\n",
    "target = target_label_name # your target name here\n",
    "if learning_type == 'regression':\n",
    "    df[target] = pd.to_numeric(df[target], errors='coerce')\n",
    "df.dropna('rows', how='any', subset=[target], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract X and y\n",
    "df_X = df.drop(columns=[target])\n",
    "df_y = df[target]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detach preprocessing pipeline (which needs to see all training data)\n",
    "preprocessor_index = -1\n",
    "preprocessing_steps = [] \n",
    "for i, step in enumerate(pipeline.steps):\n",
    "    preprocessing_steps.append(step)\n",
    "    if step[0]=='preprocessor':\n",
    "        preprocessor_index = i\n",
    "        break\n",
    "if len(pipeline.steps) > preprocessor_index+1 and pipeline.steps[preprocessor_index + 1][0] == 'cognito':\n",
    "    preprocessor_index += 1\n",
    "    preprocessing_steps.append(pipeline.steps[preprocessor_index])\n",
    "if preprocessor_index >= 0:\n",
    "    preprocessing_pipeline = Pipeline(memory=pipeline.memory, steps=preprocessing_steps)\n",
    "    pipeline = Pipeline(steps=pipeline.steps[preprocessor_index+1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda3/lib/python3.7/site-packages/sklearn/base.py:467: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/pipeline.py:451: RuntimeWarning: invalid value encountered in sqrt\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    }
   ],
   "source": [
    "# Preprocess X\n",
    "# preprocessor should see all data for cross_validate on the remaining steps to match autoai scores\n",
    "known_values_list.clear()  #  known_values_list is filled in by the preprocessing_pipeline if needed\n",
    "preprocessing_pipeline.fit(df_X.values, df_y.values)\n",
    "X_prep = preprocessing_pipeline.transform(df_X.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Split data into Training and Holdout sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_type specified as: classification\n"
     ]
    }
   ],
   "source": [
    "# determine learning_type and perform holdout split (stratify conditionally)\n",
    "if learning_type is None:\n",
    "    # When the problem type is not available in the metadata, use the sklearn type_of_target to determine whether to stratify the holdout split\n",
    "    # Caution:  This can mis-classify regression targets that can be expressed as integers as multiclass, in which case manually override the learning_type\n",
    "    from sklearn.utils.multiclass import type_of_target\n",
    "    if type_of_target(df_y.values) in ['multiclass', 'binary']:\n",
    "        learning_type = 'classification'\n",
    "    else:\n",
    "        learning_type = 'regression'\n",
    "    print('learning_type determined by type_of_target as:',learning_type)\n",
    "else:\n",
    "    print('learning_type specified as:',learning_type)\n",
    "    \n",
    "from sklearn.model_selection import train_test_split\n",
    "if learning_type == 'classification':\n",
    "    X, X_holdout, y, y_holdout = train_test_split(X_prep, df_y.values, test_size=holdout_fraction, random_state=random_state, stratify=df_y.values)\n",
    "else:\n",
    "    X, X_holdout, y, y_holdout = train_test_split(X_prep, df_y.values, test_size=holdout_fraction, random_state=random_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Additional setup: Define a function that returns a scorer for the target's positive label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to produce a scorer for a given positive label\n",
    "def make_pos_label_scorer(scorer, pos_label):\n",
    "    kwargs = {'pos_label':pos_label}\n",
    "    for prop in ['needs_proba', 'needs_threshold']:\n",
    "        if prop+'=True' in scorer._factory_args():\n",
    "            kwargs[prop] = True\n",
    "    if scorer._sign == -1:\n",
    "        kwargs['greater_is_better'] = False\n",
    "    from sklearn.metrics import make_scorer\n",
    "    scorer=make_scorer(scorer._score_func, **kwargs)\n",
    "    return scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Fit pipeline, predict on Holdout set, calculate score, perform cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('estimator', GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=0.9950479310055053, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_...    subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the remainder of the pipeline on the training data\n",
    "pipeline.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on the holdout data\n",
    "y_pred = pipeline.predict(X_holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8946360153256705\n"
     ]
    }
   ],
   "source": [
    "# compute score for the optimization metric\n",
    "# scorer may need pos_label, but not all scorers take pos_label parameter\n",
    "from sklearn.metrics import get_scorer\n",
    "scorer = get_scorer(optimization_metric)\n",
    "score = None\n",
    "#score = scorer(pipeline, X_holdout, y_holdout)  # this would suffice for simple cases\n",
    "pos_label = None  # if you want to supply the pos_label, specify it here\n",
    "if pos_label is None and 'pos_label' in _input_metadata:\n",
    "    pos_label=_input_metadata['pos_label']\n",
    "try:\n",
    "    score = scorer(pipeline, X_holdout, y_holdout)\n",
    "except Exception as e1:\n",
    "    if pos_label is None or str(pos_label)=='':\n",
    "        print('You may have to provide a value for pos_label in order for a score to be calculated.')\n",
    "        raise(e1)\n",
    "    else:\n",
    "        exception_string=str(e1)\n",
    "        if 'pos_label' in exception_string:\n",
    "            try:\n",
    "                scorer = make_pos_label_scorer(scorer, pos_label=pos_label)\n",
    "                score = scorer(pipeline, X_holdout, y_holdout)\n",
    "                print('Retry was successful with pos_label supplied to scorer')\n",
    "            except Exception as e2:\n",
    "                print('Initial attempt to use scorer failed.  Exception was:')\n",
    "                print(e1)\n",
    "                print('')\n",
    "                print('Retry with pos_label failed.  Exception was:')\n",
    "                print(e2)\n",
    "        else:\n",
    "            raise(e1)\n",
    "\n",
    "if score is not None:\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90978886756238"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cross_validate pipeline using training data\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "if learning_type == 'classification':\n",
    "    fold_generator = StratifiedKFold(n_splits=cv_num_folds, random_state=random_state)\n",
    "else:\n",
    "    fold_generator = KFold(n_splits=cv_num_folds, random_state=random_state)\n",
    "cv_results = cross_validate(pipeline, X, y, cv=fold_generator, scoring={optimization_metric:scorer}, return_train_score=True)\n",
    "import numpy as np\n",
    "np.mean(cv_results['test_' + optimization_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.42750883, 0.46174741, 0.54150486]),\n",
       " 'score_time': array([0.00514698, 0.0048337 , 0.00717092]),\n",
       " 'test_accuracy': array([0.91234805, 0.91106846, 0.9059501 ]),\n",
       " 'train_accuracy': array([0.91810621, 0.92258477, 0.92354447])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
